# 概要设计
技术挑战包括：如何不重复地获取并存储全网海量 URL？如何保证爬虫可以快速爬取全网网页但又不会给目标网站带来巨大的并发压力（避免被风控和击溃服务器）？
## 实现原理
如何不重复地获取并存储全网海量 URL？
1. 人工收集所有知名网站的url，作为爬虫的种子url。（解决冷启动）
2. 爬取种子url的html页面，从爬取的html中解析出关联的url，作为后续要爬取的url，依次循环下去，就能获取到全网海量url。

如何保证爬虫可以快速爬取全网网页但又不会给目标网站带来巨大的并发压力（避免被风控和击溃服务器）？
对同一个域名的请求，限制并发数，甚至可以在连续两次请求间设置等待时间。

## 简要架构
![Resilience](./../pictures/bot/brief.drawio.png)

# 详细设计

## 总体架构


# 设计小结

# 大数据存储系统
## HDFS
设计目标：存储非常大的文件，比如几百M、G、或者TB级别。
HDFS是一个分布式文件系统，采用主从架构，主要由NameNode和DataNode两类节点组成，NameNode是HDFS的主节点，负责管理文件系统的命名空间和文件快的映射关系。
它存储所有文件和目录的元数据（如文件名、权限、块位置等），并协调客户端对数据的访问请求。DataNode是HDFS的工作节点，负责存储实际的数据块。
具有以下特点：
1. 高容错：副本机制
2. 高吞吐：批量
3. 可扩展性：添加节点，轻松扩展HDFS的存储容量
4. **高延时**
5. 文件规模：支持**数亿个文件**。一个文件/目录/文件一般占有150字节的元数据内存空间，如果有100W个文件，每个文件占用1个文件快，则需要大约300M的内存，，因此十亿级别的文件数量在现有商用机器上难以支持。

### HDFS的工作原理
HDFS通过分布式存储和冗余机制，实现高可靠性和高可用性。

文件存储：HDFS将文件分割成固定大小的块（默认64MB或128MB），并将这些块存储在不同的DataNode上。每个块会被复制到多个DataNode，以确保数据的可靠性。

数据写入：当客户端向HDFS写入数据时，数据首先被分割成块，并通过Pipeline机制写入到多个DataNode。具体步骤如下：
1. 客户端请求NameNode：客户向NameNode请求写入文件
2. NameNode分配块和DataNode：NameNode为文件分配数据块并选择存储这些块的DataNode
3. 客户端写入数据块：客户端将数据块写入第一个DataNode，第一个DataNode再将数据块复制到第二个DataNode，以此类推
4. 数据块确认：当所有副本写入成功后，客户端接收到确认消息，表示数据写入完成。

数据读取：当客户端从HDFS读取数据时，NameNode提供数据块的位置信息，客户端直接从相应的DataNode读取数据。具体步骤如下：
1. 客户端请求NameNode：客户向NameNode请求读取文件
2. NameNode返回块位置：NameNode返回文件块所在的DataNode列表
3. 客户端读取数据块：客户端直接从DataNode读取数据块，并在本地合并这些数据块，恢复成完整的文件。

### 应用场景
数据仓库、日志存储与处理、多媒体存储（如图像、音频和视频文件等）。

HDFS适合大数据批处理，不擅长低延迟访问。对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时，HBase更适合低延时的数据访问。

### 不适用的场景
1. 低延时的数据访问：对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时HBase更适合低延时的数据访问。
2. 大量小文件：文件的元数据（如目录结构，文件block的节点列表，block-node mapping）保存在NameNode的内存中， 整个文件系统的文件数量会受限于NameNode的内存大小。 经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。
  ![Resilience](./../pictures/network_storage/img_4.png)
3. 多方读写，需要任意的文件修改。HDFS采用追加（append-only）的方式写入数据。不支持文件任意offset的修改。不支持多个写入器（writer）。

### 操作命令
（put)将本地文件上传到HDFS： hdfs dfs -put 本地文件  **/路径**

（get)从HDFS下载文件到本地： hdfs dfs -get **/路径** 本地文件

⚠️注意：文件系统HDFS的存储的key是文件路径

## HBase
HBase是一个分布式的基于列式存储的数据库，它采用hdfs存储，zookeeper进行管理。HBase虽然还是以HDFS作为文件存储，但是它存储的数据不再是简单的文本文件，
而是经过HBase优化压缩过的二进制文件，所以它的存储文件通常是不能够直接查看的。
### 数据模型
NameSpace：类似于关系型数据库的DataBase概念，咩哥命名空间下有多个表。

Table：类似于关系型数据库的表的概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态，按需指定。因此，和SQL数据相比，HBase能轻松应对字段变更的场景。

Column：HBase中每个列都由Column Family（列族）和Column Qualifier（列限定符）进行限定。建表时，只需指明列族，而列限定符无需预先定义。

Timestamp：用于表示数据的不同版本，每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段。

Cell：由{rowkey, column Family:column Qualifier, timestamp} 唯一确定的单元，cell中的数据是没有类型的，**全部是字节码形式存储**。

![Resilience](./../pictures/network_storage/img_1.png)

### 架构
HBase是主从架构，Master作为主节点，Regionserver作为从节点
![Resilience](./../pictures/network_storage/img_2.png)
Zookeeper： Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等

HDFS： 提供底层数据支撑

Master： 监控RegionServer，进行负载均衡；
RegionServer故障转移；
处理元数据变更；
处理region的分配或分裂；
处理增删改查请求；

RegionServer：负责存储HBase的实际数据；
管理分配给它的Region；
刷新缓存到HDFS；
维护Hlog；
执行压缩；
负责处理Region分片；

Region： 实际存储数据，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region

Hlog： 预写日志WAL，用于记录HBase的修改记录

MemStore： 写缓存

StoreFile： 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的

### HBase特点
1. 大： 一个表可以有**数十亿行**，上百万列；
2. 无模式： 每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列；
3. 面向列： 面向列（族）的存储和权限控制，列（族）独立检索；
4. 稀疏： 空（null）列并不占用存储空间，表可以设计的非常稀疏；
5. 数据多版本： 每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元 格插入时的时间戳；
6. 数据类型单一： Hbase 中的数据都是二进制存储，没有类型。

### HBase性能
1. HBase集群的写操作吞吐量最大可以达到70000+ ops/sec，延迟在几个毫秒左右。
2. HBase集群的读操作吞吐量最大可以达到26000+ ops/sec，单台机器的吞吐量可以达到8000+ ops/sec，延迟在几毫秒到20毫秒左右‌.
3. HBase每个Cell的最大为10MB（默认）

### HBase操作
![Resilience](./../pictures/network_storage/img3.png)
![Resilience](./../pictures/network_storage/img_3.png)


### HBase适用场景
HBase虽然是基于HDFS来存储数据，但是他存储的数据都是经过自己优化索引后的数据，所以它对数据的存储是非常高效的。HBase以类似Redis的列式存储来管理数据，对数据的增删改查都非常高效，可以达到ms级别。
所以它完全可以作为传统意义上的数据库使用，适用于**大部分的OLTP的场景**。

## Ceph
旨在提供**高性能**、高可用性和高可扩展性的存储服务。与传统的存储系统不同，Ceph将数据分散存储在多个节点上，并通过复制和冗余机制确保数据的安全性和可靠性。Ceph的优点在于它能够提供高性能的存储服务，并且可以轻松扩展到大量节点。

对象存储：也就是**通常意义的键值存储**，其接口就是简单的GET、PUT、DEL 和其他扩展，代表主要有 Swift 、S3 以及 Gluster 等。

Ceph也支持块存储，和文件系统存储。

Ceph适合存储海量中小文件，典型场景：云相册。
Ceph能扩展到支持100亿个对象，并提供良好且可预测的性能。 读写性能跟HBase差不多
ref: https://blocksandfiles.com/2020/09/22/ceph-scales-to-10-billion-objects/

### Ceph适用场景
1. 低延时，实时存储系统
2. 具有超大数量小文件处理能力

# 大数据存储比较
高性能： Ceph、HBase

OLTP：Ceph、HBase

OLAP：HDFS、HBase

规模大小： Ceph （百亿kv） > HBase(数十亿行) > HDFS(数亿文件)。
因为ceph的元数据节点有多个，而HDFS的元数据节点只有一个（主备）

HDFS适合存储大文件（>1G)

Ceph VS HBase（对象存储与HBase）
1. Ceph对象存储，所谓对象存储其实就是kv存储，所以文件、图片等存储就是典型的kv存储场景，很适合Ceph对象存储，或者S3。而且对象存储（Ceph和S3等）适合存储中小文件（<=100M)
2. HBase可以存储几百列，Hbase适合存储非kv结构，有多个字段的场景。（当然也可以kv，其实就是只有两个列）

分片的关系数据库适合存储文件元数据表。

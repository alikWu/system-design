# 设计不足之处
1. 没有考虑到用户文件存储也是需要拆分，因为用户可能上传一个很大的文件，比如100G的文件，大部分存储系统应该都做不到直接将这么大的数据直接存储。就算存储系统能支持，但这么大的数据在传输中，或者存储系统保存的时候出现一点小问题，就需要将这么大的数据重新再是一次（因为存储系统一般都没有断点续传的功能），容易耗尽系统资源。
2. 服务端不应该处理文件拆分/合并的操作，因为client应用端也有拆分逻辑，server端有合并逻辑，造成了client端和server端逻辑冗余，且难维护。
3. 对各种大规模数据存储系统，不是很了解其各自的特性，存储选型没有经过调研而匆忙决定。
4. 限速设计的方式会对系统资源造成浪费，不应该对一个正持有大文件的线程休眠，应该等线程释放大文件资源后再休眠。
5. 系统可用性不足，对于资源紧张（付费）的系统，应该合理隔离各个用户的资源使用，避免个别用户影响大部人用户的体验，应该进行用户维度的限流。
6. 缺少对用户行为的监控与分析，比如热点文件。

# system redesign
为了解决之前的设计中存在的问题，这次在原来的设计上重新修改了设计方案，修改后的系统架构图如下所示：
![Resilience](./../pictures/network_storage/redesign.drawio.png)

通过对HDFS，HBase，Ceph等分布式存储系统进行分析后发现，其实HDFS是最符合这个系统的使用场景的，但是如果使用HDFS了，那么可能这个设计题就没有出的必要了。至于HBase和Ceph，其实这两个都可以满足要求，只是HBase相对来说更熟悉些。


改动主要有以下几点：
1. 文件的拆分与组装全部由client端app来完成，服务端只负责告诉clients端其目标拆分小文件数量。约定每个拆分文件大小为10M。
2. 增加了用户文件元数据表，该表保存了用户文件与拆分后各个小文件的映射关系，具体数据模型展示在本文最后。
3. 限速由server端来控制：一是通过用户上传/下载文件到HBase/MQ中的并发度来控制， 二是通过每个线程中发送并释放一个小的拆分文件后，是否休眠来控制速度。
4. 防止个别用户影响系统，在API Gateway引入了Hystrix来实现对用户维度的访问限流，以及总流量的限流。
5. 引入CDN技术，来对热点文件分发，减少服务器压力。（但**会有数据安全性的风险，因为CDN侧能看到解密后的文件**的）
6. **数据倾斜治理**。新增了用户行为分析服务，通过对用户访问文件行为的异步埋点记录，分析出热点文件，将热点文件提前加载到CDN中。
7. 去除了DB Reader组件，直接由Downloader来读取HBase。

文件下载流程：
1. clients端获取所有文件列表，从中选择要下载的文件，点击下载.
2. 如果是热点文件，则clients访问CDN来下载文件（CDB没有则会回源）；如果非热点文件，则直接访问server端进行下载。
3. Downloader在访问Meta Service获取用户权限、服务等级、以及文件拆分等元数据后，如果没有异常，就开始通过DB Reader读取HBase，进行文件下载。
4. 同时，Downloader会将用户此次下载行为异步记录到Analyser中，用于进行热点文件分析。
5. clients在收到文件后，按规则将文件拼接成原始文件。

文件上传流程：
1. clients端首先确认文件是否已存储在服务器上，如果已存储，则只需要将用户与文件的映射关系通过meta service进行保存，就完成了文件上传。否则，进行下一步。
2. clients根据服务器的要求进行文件拆分，并将拆分后的文件依次上传至Uploader。
3. Uploader将文件发送至MQ，并将用户，文件相关的信息保存到Meta service。完成后，就可以返回用户，文件上传成功。
4. DB writer会将MQ中的文件按要求保存至HBase中。


# Q&A
Q1：为什么要有MQ的存在？
1. MQ主要是基于稳定性考虑，防止突发流量将DB击溃; 
2. 为了性能，文件只要成功发送到了MQ，就可以认为文件已经上传成功了，其余的耗时操作可以放到后台执行。
3. 解耦uploader和db writer，符合单一职责原则。

Q2: 为什么加密在MQ后面？

为了性能，文件只用上传到MQ就可以了，加密等耗时操作可以放到后台执行

Q3：为什么查询服务要去除DB Reader？

存在的意义不大，对系统性能、可用性等没啥提升，反而让系统架构变得复杂。

# 补充
文件元数据表如下所示：
|file_id|file_md5|split_num|
|--|----|----|
|xxxx|xxxx|xx|




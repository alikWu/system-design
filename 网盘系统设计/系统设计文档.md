# 概要设计

## 实现原理
1. 可以通过HTTP1.1 长连接协议来实现文件上传和下载 (chunked模式)
2. 断点续传功能可以通过已下载文件的offset来实现。
3. 共享功能可以通过用户分享一个唯一ID来实现，被分享的用户可以拿着这个ID是下载分享用户的共享文件。
4. 流速控制可以通过控制用户上传/下载文件中的相邻两部分数据的间隔时间来控制流速。如果是vip用户，可以把间隔时间调的更低，或者取消，甚至可以为vip用户单独搭建特殊集群。
5. 秒传功能可以通过md5等hash函数来判断文件是否已存在云盘，从而实现秒传。

## 简要架构

![Resilience](./../pictures/network_storage/brief.drawio.png)

# 详细设计

## 存储设计

理论上，总存储空间估算为 10 亿 TB，即 1 万亿 GB。

```shell
10亿×1TB=10亿TB
```

但考虑到大多数用户并不会完全用掉这个空间，还有很多用户存储的文件其实是和别人重复的（电影、电子书、软件安装包等），真正需要的存储空间大约是这个估算值的
10%，即 1 亿 TB。

因为是TB级别大规模数据存储，且要求存储可靠性达到6个9的考虑，因此，采用高可用分布式存储系统NoSQL HBase来存储用户文件。

数据模型如下所示
|file_id|file_data|file_md5|
|----|----|----|
|xxx|xxxxx|xxx|

因为存储可靠性要求达到 99.9999% ，所以增加个备用机房，为数据做异地备份，两个机房之间用两条不同路线的光纤连接，同步数据。在主库机房出现故障时，通过DNS设置将流量切换到备用机房，设计图如下：
![Resilience](./../pictures/network_storage/db.drawio.png)

## meta service

meta service的主要功能有：
1. 管理用户访问文件的权限
2. 管理用户上传/下载速度
3. 管理用户共享文件下载

数据模型如下所示
|user_id|file_name|file_id|rate_limit_level|upload_rate_limit|download_rate_limit|shared_id|
|----|----|----|----|----|----|----|
|xxx|xxxx|xxxx|xxxx|xxxx|xxxx|xxxx|

基于用户量庞大，且为了减少不同技术栈的使用，便于运维管理，于是也选用HBase来存储用户权限信息。

meta service的设计如下图所示：
![Resilience](./../pictures/network_storage/meta.drawio.png)

## upload service
因为要对上传限速，所以需要client端，也就是app或者web前端将文件分片后再上传。 upload service每接收到一个文件分片，就休眠一会（eg：100ms），通过这种方式来对上传进行限速了。
其实，这种通过服务端对上传限速的方式容易导致超时重试，从而对网络造成额外的负担。可以让客户端来限速，每上传完成一分片的数据，就休眠一会，再上传，这种方式安全性可能没那么好，限速容易被破解。
（实际上http协议对于大文件，如果clients端不主动将文件分片，http发送端会选择chunked或者content-length方式来分片上传文件，但是http在接收端是会将文件拼接好之后才会提交给接收端的，这样就不好对上传进行限速。）

upload service不仅要上传文件，还要将文件的访问者权限等写入meta service中，从而实现访问权限控制。

秒传功能通过判断上传文件的md5是否已存储在HBase中来实现。如果md5已存在HBase中，则表示该文件已上传，不需要重复上传该文件，只需要将用户的访问权限与文件信息写入meta service中就可以了。

![Resilience](./../pictures/network_storage/upload.drawio.png)

## download service
![Resilience](./../pictures/network_storage/download.drawio.png)

共享文件下载通过用户上传的共享ID来判断，该文件是否是文件owner共享出来的ID，如果是，则表示拥有该ID的用户也有下载该文件的权限。

下载速度限制也是通过在server端每发送一分片数据，休眠一会的方式来实现。

## 总体架构
![Resilience](./../pictures/network_storage/final.drawio.png)

断点续传(下载）则是通过offset来实现，每次download service返回给client一部分数据，也会把该数据对应的offset一起返回给client，这样客户端就能知道当前已下载的进度offset，
如果有问题下载被中断了，等下次重新开始下载的时候，客户端可以将该offset上传只server端，download service就会根据offset来返回未下载完成的部分。

同样的，关于断点上传也是类似，每次clients端上传的时候都上传一分片数据，文件的md5， offset，以及目标server IP。这样就能保障同一台服务器来处理该文件的上传请求。 
# 架构小结
对于功能需求，我们通过upload service和download service实现了文件的上传和下载；通过offset的方式实现了断点续传；通过共享文件的share_id实现了文件的共享需求；通过引入休眠机制对上传和下载进行流速控制；通过文件的md5校验实现了"秒传"功能。

对于非功能需求，
1. 规模大小：使用NoSQL HBase支持了用户TB级别的存储规模；对于高并发读写（TPS = 10000， QPS=10000），upload service和download service都是无状态的，可以通过水平扩展支持高并发（比如服务器都扩展至100台），因为DB选用的是HBase，对这点流量也是足够支持的。
2. 高并发：对于高并发读写（TPS = 10000， QPS=10000），upload service和download service都是无状态的，可以通过水平扩展支持高并发（比如服务器都扩展至100台），因为DB选用的是HBase，对这点流量也是足够支持的。
3. 存储高可靠：HBase本身是一个高可靠的分布式存储系统，单个HBase集群就可实现99.99%的高可靠，再加上我们采用异地多机房备份机制，就算单个集群HBase出现故障，也不会出现丢失数据的事故，可满足99.9999% 的高可靠。
4. 服务高可用：我们的服务器都是无状态的，采用集群方式部署，不会因为某一台服务器出现故障而影响用户使用，可以随时根据需要水平扩展服务器数量。同时API Gateway也是无状态的，集群方式部署，也不存在单点故障的问题。同时，我们采用了GSLB技术来保障了HBase的高可用，不会因为某一个机房断电之类的事故而影响服务正常使用。综上，服务可满足99.99%的高可用要求。
5. 安全性：通过meta service来对用户的访问进行权限控制，无法查看别人的非共享的文件。同时，也通过对称加密算法AES对文件进行加解密。


# 补充
其实，也可以采用server端push模式，
就是在client端向server端发起下载请求后，server端主动将分片数据发给client端。不过http协议不支持，http只支持单向通信。
![Resilience](./../pictures/network_storage/img.png)